{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWR-bhnG_wpQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob, gc\n",
    "from google.colab import drive\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Monter Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Chemins\n",
    "INPUT_RAW_METEO_DATA_PATH = '/content/drive/MyDrive/Stage_CEFREM_Salma_2025/Data/Meteo/InSitu/OriginalFromMeteoFrance/'\n",
    "INPUT_METADATA_WITH_GIS_PATH = '/content/drive/MyDrive/Stage_CEFREM_Salma_2025/Data/Meteo/InSitu/Consolidated_Stations_With_Raster_Values_And_Aspect_Components_StrictlyFiltered.csv'\n",
    "FINAL_CONSOLIDATED_DATA_PATH = '/content/drive/MyDrive/Stage_CEFREM_Salma_2025/Data/Meteo/InSitu/Fully_Consolidated_Stations_Meteo_GIS_Data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnTXICrVAHqq"
   },
   "outputs": [],
   "source": [
    "# Mapping colonnes fichiers bruts\n",
    "column_mapping_rrt_vent = {\n",
    "    'NUM_POSTE': 'NUM_POSTE', 'AAAAMMJJ': 'DATE', 'RR': 'RR_obs',\n",
    "    'TX': 'TX_obs', 'TN': 'TN_obs', 'TM': 'TM_obs', 'FFM': 'FFM_obs'\n",
    "    # ... ajoute d'autres colonnes au besoin\n",
    "}\n",
    "\n",
    "column_mapping_autres_params = {\n",
    "    'NUM_POSTE': 'NUM_POSTE', 'AAAAMMJJ': 'DATE',\n",
    "    'ETPMON': 'ETPMON_obs', 'HNEIGEF': 'HNEIGEF_obs'\n",
    "    # ... ajoute d'autres colonnes au besoin\n",
    "}\n",
    "\n",
    "# Réduction mémoire\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if pd.api.types.is_numeric_dtype(col_type):\n",
    "            if 'int' in str(col_type):\n",
    "                c_min, c_max = df[col].min(), df[col].max()\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            elif 'float' in str(col_type):\n",
    "                c_min, c_max = df[col].min(), df[col].max()\n",
    "                if c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "        elif col_type == 'object':\n",
    "            if df[col].nunique() / len(df[col]) < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\" Mémoire réduite de {start_mem:.2f} MB à {end_mem:.2f} MB\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niEE4oLSAKyY"
   },
   "outputs": [],
   "source": [
    "print(\"--- Étape 1 : Chargement des métadonnées des stations ---\")\n",
    "stations_metadata_df = pd.read_csv(INPUT_METADATA_WITH_GIS_PATH)\n",
    "stations_metadata_df['NUM_POSTE'] = stations_metadata_df['NUM_POSTE'].astype(str)\n",
    "stations_metadata_df = reduce_mem_usage(stations_metadata_df)\n",
    "print(f\"{len(stations_metadata_df)} stations chargées avec GIS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNJZ0Em5ANHH"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Étape 2 : Lecture des fichiers bruts ---\")\n",
    "all_raw_meteo_for_pyrenees = []\n",
    "raw_data_by_department = {}\n",
    "all_raw_csv_files = glob.glob(os.path.join(INPUT_RAW_METEO_DATA_PATH, 'Q_*.csv'))\n",
    "unique_dept_periods = sorted({(f.split('_')[1], f.split('_')[2]) for f in map(os.path.basename, all_raw_csv_files) if len(f.split('_')) >= 3})\n",
    "\n",
    "for dept_code, period in unique_dept_periods:\n",
    "    print(f\" Département {dept_code}, période {period}\")\n",
    "    file1 = os.path.join(INPUT_RAW_METEO_DATA_PATH, f\"Q_{dept_code}_{period}_RR-T-Vent.csv\")\n",
    "    file2 = os.path.join(INPUT_RAW_METEO_DATA_PATH, f\"Q_{dept_code}_{period}_autres-parametres.csv\")\n",
    "\n",
    "    df1 = pd.read_csv(file1, sep=';', on_bad_lines='skip') if os.path.exists(file1) else pd.DataFrame()\n",
    "    df2 = pd.read_csv(file2, sep=';', on_bad_lines='skip') if os.path.exists(file2) else pd.DataFrame()\n",
    "\n",
    "    if not df1.empty:\n",
    "        df1.rename(columns=column_mapping_rrt_vent, inplace=True)\n",
    "        df1['DATE'] = pd.to_datetime(df1['DATE'], format='%Y%m%d', errors='coerce')\n",
    "        df1['NUM_POSTE'] = df1['NUM_POSTE'].astype(str)\n",
    "        df1 = reduce_mem_usage(df1, verbose=False)\n",
    "\n",
    "    if not df2.empty:\n",
    "        df2.rename(columns=column_mapping_autres_params, inplace=True)\n",
    "        df2['DATE'] = pd.to_datetime(df2['DATE'], format='%Y%m%d', errors='coerce')\n",
    "        df2['NUM_POSTE'] = df2['NUM_POSTE'].astype(str)\n",
    "        df2 = reduce_mem_usage(df2, verbose=False)\n",
    "\n",
    "    merged = pd.merge(df1, df2, on=['NUM_POSTE', 'DATE'], how='outer') if not df1.empty and not df2.empty else df1 if not df1.empty else df2\n",
    "\n",
    "    if not merged.empty:\n",
    "        merged = reduce_mem_usage(merged, verbose=False)\n",
    "        if dept_code not in raw_data_by_department:\n",
    "            raw_data_by_department[dept_code] = merged\n",
    "        else:\n",
    "            raw_data_by_department[dept_code] = pd.concat([raw_data_by_department[dept_code], merged])\n",
    "            raw_data_by_department[dept_code].drop_duplicates(subset=['NUM_POSTE', 'DATE'], inplace=True)\n",
    "            raw_data_by_department[dept_code] = reduce_mem_usage(raw_data_by_department[dept_code], verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eUHGbpPAQOf"
   },
   "outputs": [],
   "source": [
    "processed_ids = set()\n",
    "\n",
    "for station_id in stations_metadata_df['NUM_POSTE'].unique():\n",
    "    dept = station_id[:2].zfill(2)\n",
    "    if dept in raw_data_by_department:\n",
    "        df_station = raw_data_by_department[dept]\n",
    "        station_data = df_station[df_station['NUM_POSTE'] == station_id]\n",
    "        if not station_data.empty:\n",
    "            all_raw_meteo_for_pyrenees.append(station_data.copy())\n",
    "            processed_ids.add(station_id)\n",
    "\n",
    "del raw_data_by_department\n",
    "gc.collect()\n",
    "\n",
    "if all_raw_meteo_for_pyrenees:\n",
    "    consolidated_meteo_df = pd.concat(all_raw_meteo_for_pyrenees, ignore_index=True)\n",
    "    consolidated_meteo_df = reduce_mem_usage(consolidated_meteo_df)\n",
    "else:\n",
    "    print(\" Aucune donnée brute trouvée pour les stations.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITtTetSRASzr"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Étape 3 : Fusion finale ---\")\n",
    "final_df = pd.merge(consolidated_meteo_df, stations_metadata_df, on='NUM_POSTE', how='left')\n",
    "del consolidated_meteo_df\n",
    "gc.collect()\n",
    "\n",
    "# Supprimer colonnes doublons venant des fichiers bruts\n",
    "cols_to_drop = [c for c in final_df.columns if c.endswith('_raw_data')]\n",
    "final_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "final_df['DATE'] = pd.to_datetime(final_df['DATE'], errors='coerce')\n",
    "final_df = reduce_mem_usage(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4IIhviyAVZ3"
   },
   "outputs": [],
   "source": [
    "final_df.to_csv(FINAL_CONSOLIDATED_DATA_PATH, index=False)\n",
    "print(f\"\\n Données finales enregistrées dans : {FINAL_CONSOLIDATED_DATA_PATH}\")\n",
    "\n",
    "print(\"\\n--- Analyse des NaN ---\")\n",
    "nan_summary = final_df.isna().sum().to_frame('NaN')\n",
    "nan_summary['%'] = (nan_summary['NaN'] / len(final_df)) * 100\n",
    "print(nan_summary[nan_summary['NaN'] > 0].sort_values(by='%', ascending=False))\n",
    "\n",
    "print(f\"\\n Fin du script : {len(processed_ids)} stations des Pyrénées traitées.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOekmKu/O+yqDW9+LtJ0h9i",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
